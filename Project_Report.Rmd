---
title: |
  ETC5543 Business Analytics Creative Activity 
  
  Project Report
date:  "`r format(Sys.time(), '%d %B, %Y')`"
author: |
  | Xianghe Xu
  |
  | 32929447
  |
  | [Project Github Repository Link](https://github.com/xxuu0086/ETC5543_Internship_Project) 
  
team: Group 1
subtitle: |
  Department of Econometrics and Business Statistics

  Monash University
  
  | - 
  
  __The Australian Comparative Study of Survey Method Data Quality Project.__
  
  | -
  
abstract: |

  This report conducts a comprehensive examination of survey response data quality through a comparison of probability-based methods with non-probability-based methods, using the ACSSM dataset. Various relevant survey response metrics are applied to the dataset variables, enabling the assessment and comparison of response data quality between probability-based and non-probability-based survey methods, particularly regarding the number of careless survey responses in each survey. The results of the comparative analysis demonstrate that probability-based methods exhibit fewer careless responses compared to non-probability methods across nearly all survey response metrics used in this project. This finding supports the hypothesis that non-probability-based methods result in poorer data quality.
  

# bibliography: bibliography.bib
header-includes: 
  - \renewcommand{\and}{\\ \\}
  - \AddToHook{env/abstract/before}{\vspace*{2cm}}
  - \makeatletter\AddToHook{cmd/@date/before}{\vspace*{2cm}}\makeatother
output:
  bookdown::pdf_document2:
    toc: FALSE
    number_sections: TRUE
    keep_md: yes
  html_document: default
   


---

\newpage

\newpage

[This report is for ETC5543 Business Analytics Creative Activity internship project by student __Xianghe Xu__, the github repository for this project is [https://github.com/xxuu0086/ETC5543_Internship_Project](https://github.com/xxuu0086/ETC5543_Internship_Project) .]{style="color:#006DAE;"}

\tableofcontents
\newpage

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,  
  warning = FALSE,  
  error = FALSE,    
  out.width = "70%",
  fig.align = "center",
  fig.width = 8, 
  fig.height = 7,
  fig.retina = 3,
  fig.pos = "H", 
  out.extra = "",
  dev = "png")
```

```{r}
# Load packages and data
library(tidyverse)
library(haven)
library(shiny)
library(kableExtra)
library(gridExtra)
library(naniar)

dat <- haven::read_sav("../Data and survey materials/ACSSM_Merged.sav")
```

```{r}
dat1 <- dat %>% 
  mutate(prob_based = ifelse(DataSource <= 5, "Probablity based", "Non-probablity based")) %>% # Indicate probability based
  filter(DataSource %in% c(2, 5, 6, 7, 8, 9)) # Filter data source

```


```{r}
# Mutate new column with DataSource name
dat1 <- dat1 %>% 
  mutate(DataSource_name = case_when(
    DataSource == 2 ~ "Life in Australia",
    DataSource == 5 ~ "SMS push to web",
    DataSource == 6 ~ "Panel 1",
    DataSource == 7 ~ "Panel 2",
    DataSource == 8 ~ "Panel 3",
    DataSource == 9 ~ "Panel 4")) %>% 
  mutate(DataSource_name = factor(DataSource_name,
                       levels = c("Life in Australia",
                                  "SMS push to web",
                                  "Panel 1",
                                  "Panel 2",
                                  "Panel 3",
                                  "Panel 4"),
                       labels = c("Life in Australia",
                                  "SMS push to web",
                                  "Panel 1",
                                  "Panel 2",
                                  "Panel 3",
                                  "Panel 4"))) # Use str() to check levels
```

# Introduction and Motivation

In the age of big data, survey research stands out as a fundamental tool for gathering valuable insights from the population. The quality of the responses collected holds paramount importance in making informed decisions. Recent advancements in technology and communication methods have led to a rise in surveys that assessible exclusively to specific groups through online platforms and social media. These surveys are designed for particular purposes, offering less costly and time-efficient data collection. This approach is commonly known as non-probability-based survey methods, in contrast to surveys that aim to collect data from the entire population, referred to as probability-based methods.

Probability-based and non-probability-based survey methods represent two distinct approaches to data collection, prompting our curiosity about the data quality associated with each. However, response quality in non-probability online panels has received limited attention, and there is a scarcity of research on the performance of non-probability-based methods. The study in this project sets out to address the fundamental question: How do probability-based and non-probability-based survey methods compare in terms of data quality?

To investigate this question, the Social Research Centre has conducted a study in 2022 known as the Australian Comparative Study of Survey Method (ACSSM). This study is the first of its kind in Australia, delving into the examination of probability-based and non-probability-based survey methods. The primary aim of this study is to evaluate contemporary and emerging practices for general population surveys. It involves using different survey methods and approaches to collect survey data for the ACSSM study. 

In this project, survey data will be drawn from the ACSSM study. The principal aim of this project is to employ a variety of survey response metrics, including individual assessments of metrics such as speeding, straight-lining, outlier analysis, and more. The objective is to conduct a comprehensive comparison of survey response quality between probability-based and non-probability-based survey panels. This comprehensive evaluation will encompass assessments of response completeness, accuracy, reliability, and an overall comparison within both probability-based and non-probability-based surveys.

By consolidating the results from each metric and aggregating the outcomes of all metrics, we will draw a final conclusion regarding the comparison of data quality between probability-based and non-probability-based methods. The ultimate goal is to find evidence that support the hypothesis suggesting that non-probability-based panels exhibit worse data quality when compared to probability-based panels. 

# Survey response metrics

Surveys are fundamental tools for collecting data, whether from the entire population or a specific group, for experimental or non-experimental research. In real life, we've all experienced or completed surveys in various aspects of our lives, such as student surveys or experience surveys etc... There are numerous incentives that motivate people to participate in surveys. Some do it to contribute to research or a study, while others may be requested by organizations. Some participate for a chance to win prizes or receive cashback after completing the survey. These reasons drive individuals to complete surveys. However, this poses a challenge as some respondents may rush through surveys, investing minimum amount of attention and effort.

This issue is particularly relevant for self-report data collections, commonly used in both experimental and non-experimental psychology, often gathered through online measures. Invalid data can be present in such collections for various reasons, with one major factor being careless or insufficient effort (C/IE) in responding (Curran, 2016).

The primary objective of this project is to compare the data quality of surveys between probability-based and non-probability-based methods. This raises the question of what constitutes data quality and how it can be measured across different surveys. Survey response metrics provide the answer. To measure data quality, the first step is to use these survey response metrics to identify careless or insufficient effort responses in each survey method. A survey with a low number of careless responses indicates that most participants made a genuine effort to answer the questions carefully, implying high data quality. Conversely, a survey with a high number of careless responses suggests that participants did not pay much attention and may have randomly answered questions, indicating lower data quality. Therefore, the number of careless responses in a survey becomes an important indicator for comparing data quality across different survey methods.

To initiate this project, it's essential for me to gain a comprehensive understanding of the definitions and practical applications of the available survey response metrics. This entails not only comprehending the significance of each metric but also mastering the techniques for their implementation. My objective is to skillfully apply these metrics to the ACSSM dataset, as well as various variables within it, in order to identify careless responses. Simultaneously, I need to remain aware of the limitations associated with these metrics.

In my journey to learn and master these metrics, I've taken the opportunity to review several articles dedicated to survey response quality. These articles have provided valuable insights into the key metrics that are essential for our project. I have summarized some of the key metrics as follows:


With these metrics now at the forefront of our considerations, we can proceed to conduct a thorough examination of the ACSSM dataset.




# Data Description
- Describe the orginal data sets, should check outlier and missing values.
- check missings 
- Remove variables, by how many, 
- decision rule 
- description of new and cleaned dataset

# Chanllenges
- Understand the metrics
- matching the metrics to variables

# Preliminary Analysis

# Analysis

# Limitation

# Summary

\newpage

# Reference

__Citation of R package__

Wickham H, Averick M, Bryan J, Chang W, McGowan LD, Fran√ßois R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL,
  Miller E, Bache SM, M√ºller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019).
  ‚ÄúWelcome to the tidyverse.‚Äù _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686
  <https://doi.org/10.21105/joss.01686>.
  
Xie Y (2023). _bookdown: Authoring Books and Technical Documents with R Markdown_. R
  package version 0.35, <https://github.com/rstudio/bookdown>.
  
\newpage

# Appendix: All code for this report

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
