---
title: |
  ETC5543 Business Analytics Creative Activity 
  
  Project Report
date:  "`r format(Sys.time(), '%d %B, %Y')`"
author: |
  | Xianghe Xu
  |
  | 32929447
  |
  | [Project Github Repository Link](https://github.com/xxuu0086/ETC5543_Internship_Project) 
  
team: Group 1
subtitle: |
  Department of Econometrics and Business Statistics

  Monash University
  
  | - 
  
  __The Australian Comparative Study of Survey Method Data Quality Project.__
  
  | -
  
abstract: |

  This report conducts a comprehensive examination of survey response data quality through a comparison of probability-based methods with non-probability-based methods, using the ACSSM dataset. Various relevant survey response metrics are applied to the dataset variables, enabling the assessment and comparison of response data quality between probability-based and non-probability-based survey methods, particularly regarding the number of careless survey responses in each survey. The results of the comparative analysis demonstrate that probability-based methods exhibit fewer careless responses compared to non-probability methods across nearly all survey response metrics used in this project. This finding supports the hypothesis that non-probability-based methods result in poorer data quality.
  

# bibliography: bibliography.bib
header-includes: 
  - \renewcommand{\and}{\\ \\}
  - \AddToHook{env/abstract/before}{\vspace*{2cm}}
  - \makeatletter\AddToHook{cmd/@date/before}{\vspace*{2cm}}\makeatother
output:
  bookdown::pdf_document2:
    toc: FALSE
    number_sections: FALSE
    keep_md: yes
  html_document: default
   


---

\newpage

\newpage

[This report is for ETC5543 Business Analytics Creative Activity internship project by student __Xianghe Xu__, the github repository for this project is [https://github.com/xxuu0086/ETC5543_Internship_Project](https://github.com/xxuu0086/ETC5543_Internship_Project) .]{style="color:#006DAE;"}

\tableofcontents
\newpage

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,  
  warning = FALSE,  
  error = FALSE,    
  out.width = "70%",
  fig.align = "center",
  fig.width = 8, 
  fig.height = 7,
  fig.retina = 3,
  fig.pos = "H", 
  out.extra = "",
  dev = "png")
```

```{r}
# Load packages and data
library(tidyverse)
library(haven)
library(shiny)
library(kableExtra)
library(gridExtra)
library(naniar)

dat <- haven::read_sav("../Data and survey materials/ACSSM_Merged.sav")
```


# Introduction and Motivation

In the age of big data, survey research stands out as a fundamental tool for gathering valuable insights from the population. The quality of the responses collected holds paramount importance in making informed decisions. Recent advancements in technology and communication methods have led to a rise in surveys that assessible exclusively to specific groups through online platforms and social media. These surveys are designed for particular purposes, offering less costly and time-efficient data collection. This approach is commonly known as non-probability-based survey methods, in contrast to surveys that aim to collect data from the entire population, referred to as probability-based methods.

Probability-based and non-probability-based survey methods represent two distinct approaches to data collection, prompting our curiosity about the data quality associated with each. However, response quality in non-probability online panels has received limited attention, and there is a scarcity of research on the performance of non-probability-based methods. The study in this project sets out to address the fundamental question: How do probability-based and non-probability-based survey methods compare in terms of data quality?

To investigate this question, the Social Research Centre has conducted a study in 2022 known as the Australian Comparative Study of Survey Method (ACSSM). This study is the first of its kind in Australia, delving into the examination of probability-based and non-probability-based survey methods. The primary aim of this study is to evaluate contemporary and emerging practices for general population surveys. It involves using different survey methods and approaches to collect survey data for the ACSSM study. 

In this project, survey data will be drawn from the ACSSM study. The principal aim of this project is to employ a variety of survey response metrics, including individual assessments of metrics such as speeding, straight-lining, outlier analysis, and more. The objective is to conduct a comprehensive comparison of survey response quality between probability-based and non-probability-based survey panels. This comprehensive evaluation will encompass assessments of response completeness, accuracy, reliability, and an overall comparison within both probability-based and non-probability-based surveys.

By consolidating the results from each metric and aggregating the outcomes of all metrics, we will draw a final conclusion regarding the comparison of data quality between probability-based and non-probability-based methods. The ultimate goal is to find evidence that support the hypothesis suggesting that non-probability-based panels exhibit worse data quality when compared to probability-based panels. 

# Survey response metrics

Surveys are fundamental tools for collecting data, whether from the entire population or a specific group, for experimental or non-experimental research. In real life, we've all experienced or completed surveys in various aspects of our lives, such as student surveys or experience surveys etc... There are numerous incentives that motivate people to participate in surveys. Some do it to contribute to research or a study, while others may be requested by organizations. Some participate for a chance to win prizes or receive cashback after completing the survey. These reasons drive individuals to complete surveys. However, this poses a challenge as some respondents may rush through surveys, investing minimum amount of attention and effort.

This issue is particularly relevant for self-report data collections, commonly used in both experimental and non-experimental psychology, often gathered through online measures. Invalid data can be present in such collections for various reasons, with one major factor being careless or insufficient effort (C/IE) in responding (Curran, 2016). In this project, the terms "careless responding" and "insufficient effort responding" will be used interchangeably. They have the same meaning, which implies that the survey responses provided by the respondents lack sufficient effort and attention when completing the surveys.

The primary objective of this project is to compare the data quality of surveys between probability-based and non-probability-based methods. This raises the question of what constitutes data quality and how it can be measured across different surveys. Survey response metrics provide the answer. To measure data quality, the first step is to use these survey response metrics to identify careless or insufficient effort responses in each survey method. A survey with a low number of careless responses indicates that most participants made a genuine effort to answer the questions carefully, implying high data quality. Conversely, a survey with a high number of careless responses suggests that participants did not pay much attention and may have randomly answered questions, indicating lower data quality. Therefore, the number of careless responses in a survey becomes an important indicator for comparing data quality across different survey methods.

To initiate this project, it's essential for me to gain a comprehensive understanding of the definitions and practical applications of the available survey response metrics. This entails not only comprehending the significance of each metric but also mastering the techniques for their implementation. My objective is to skillfully apply these metrics to the ACSSM dataset, as well as various variables within it, in order to identify careless responses. Simultaneously, I need to remain aware of the limitations associated with these metrics.

In my journey to learn and master these metrics, I've taken the opportunity to review several articles dedicated to survey response quality. These articles have provided valuable insights into the key metrics that are essential for our project. I have summarized some of the key metrics as follows:

__Response time analysis__

- Response time analysis, often referred to as speeding, is a metric that focuses on the time taken to complete a survey or answer questions in a section of a survey. This metric typically relies on page time, which is the duration between the initiation and submission of each survey page or section online. Specifically, it examines instances of extremely small page time values, as such values indicate minimal cognitive processing of survey content. Moreover, this metric involves the use of a "cutoff" time, which is a manually set threshold. Respondents whose page time falls below this "cutoff" time are considered to be responding "carelessly."

- All the surveys in the ACSSM study have recorded the time taken by each individual to complete each section of the survey, so response time analysis can be applied to the ACSSM dataset.

__Straight lining__ 

- Straight lining, also known as response pattern analysis, refers to the tendency of respondents to select the same or very similar answer options for each item in a grid. When respondents consistently provide identical responses to a set of questions, it creates a pattern that resembles a straight line, hence the name "straight lining" for this metric. This uniformity in responses across a set of questions suggests that the survey respondents are making minimal effort when completing the survey. In practice, the straight lining are likely to be found for a set of questions with similar wording and the same answer scale. I will be looking for such questions in the ACSSM dataset.

__Consistency approach__

- This metric goes by various names, including individual consistency, inconsistent approach, psychometric antonyms, and individual reliability, and other names. Despite the different terminology, these terms all refer to the same concept, which involves assessing the consistency of survey responses provided by an individual. In essence, this approach typically involves using matched item pairs and comparing the responses to one item with the responses to another item. The matched item pairs should exhibit either a high positive correlation or a high negative correlation. Consistency in the paired responses is essential to ensure that survey respondents are providing thoughtful and careful answers. In contrast, inconsistent paired responses would suggest that respondents are answering the survey carelessly and randomly. 

__Outlier analysis__

- Outlier analysis is a metric that focuses on data points that are unusual relative to the remainder of a distribution, often seen as extremely values in a distribution. This typically occurs when individual survey respondents who are responding without sufficient effort differ from their more thoughtful counterparts.

__Item non-response__

- Item non-response or missing data in the dataset indicates that survey respondents are attempting to skip survey questions or provide non-substantive answers. This is often observed as empty answers or responses like "don't know" or "don't want to say." Respondents who tend not to provide much effort in thinking about a correct answer to a complex question are more likely to skip the question, which can be considered as "careless" respondents. 

__Midpoint selection__

- Midpoint selection is a response metric that refers to respondents selecting the middle answer from a list of answer options. The middle answer on the scale is often "Neither agree nor disagree" or "neutral." Similar to the "item non-response" metric, the behavior of choosing the middle value answer can sometimes be interpreted as respondents who are not willing to put in much effort to consider their true response to a question. Instead, they pick for the middle point, indicating neither agreement nor disagreement with the survey question. In this metric, I will focus on questions with ordinal responses, such as "Strongly agree", "Agree", "Neither agree nor disagree", "Disagree", and "Strongly disagree." These types of questions are more likely to elicit middle point selections, which can be considered a form of insufficient effort responding.

__Bogus Items__

- Bogus items are questions that have an obvious correct answer, making an incorrect response indicative of inattention to the question. If a survey respondent fails to provide the correct answer to a question with an obvious solution, it implies that the respondent is responding to the survey carelessly, without properly reading the question. This metric is straightforward to implement; it involves identifying bogus items in the survey data and examining responses that deviate from the correct answer.

__Mahalanobis Distance__

- Mahalanobis distance is a metric similar to outlier analysis. Both metrics seek extreme values that deviate from the main distribution. The key difference is that Mahalanobis distance is a multivariate outlier technique, whereas outlier analysis examines single variables. This metric calculates the distance of data points from the multivariate center, allowing it to determine whether a respondent falls at the edge of the multivariate distribution formed by responses to all items. If a respondent is positioned far from the distribution in multivariate space, they may be considered unusual and potentially careless in responding their surveys. 


The descriptions above provide a brief overview of the metrics used in this project. It's important to note that there are quite many survey response metrics available, and not all of them can be applied to the ACSSM dataset in this project. Only certain metrics are suitable for this dataset, so I won't list descriptions for all of them here. Some metrics can be used multiple times within the dataset, like the consistency approach, which allows for checking the consistency of more than one pair of responses in the dataset.

With these metrics now at the forefront of our considerations, we can proceed to conduct a thorough examination of the ACSSM dataset.


# Data Description

After familiarising ourselves with some of the popular survey response metrics discussed above, we have gained an understanding of their meaning and practical implementation. Now, it's time to delve into the dataset and establish connections between these metrics and the data. The data set used in this project is sourced from the study 'The Australian Comparative Study of Survey Method'. This study encompasses survey data from eight surveys, with four of them being probability based and the remaining four non-probability based. The data set comprises 6,043 rows of observations, indicating the participation of 6,043 survey respondents in the study. It has 290 columns, implying presence of 290 variables within the data set. It's important to note that not all 290 variables are survey questions; some variables represent the survey questions posed to each respondent and record their responses. Other variables contain additional information about the surveys, such as the respondent's 'serial number', 'ID', and 'DataSource', which records additional information about the survey respondents. As discussed earlier, there are many response metrics available and not all metrics will be used in this data set, similarly, not all variables will be used to compare the quality of survey data. Therefore, before applying the metrics to the data, I must manually review each variable to identify the right ones for the metrics. Some metrics are straightforward to apply to the variable. For instance, the response time analysis metric only requires the page time. In such cases, I can directly access the variable that contains the page time which is the time taken to answer the survey questions and use it for response time analysis, thus determining whether respondents are responding quickly or not. However, some metrics are more complex to implement. For example, the consistency approach requires paired variables that exhibit high positive or negative correlations. Pairing these variables is a manual process. 

Now, let's take a quick glance at the data set to get an initial impression of its structure and determine whether any cleaning and tidying are required.

```{r miss1}
# Visualize missings in variables
dat %>%
  naniar::miss_var_summary() %>% 
  head(10) %>% 
  kable(caption = "Top 10 variables with a large number of missing values.") %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

```{r miss2}
# Count total varaible with >10% missings
miss10 <- dat %>%
  naniar::miss_var_summary() %>% 
  filter(pct_miss > 10) %>% 
  summarise("Count" = n()) %>% 
  table() %>% 
  kable(caption = "Total number of variables with more than 10% of missings") %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

Table \@ref(tab:miss1) displays the top 10 variables with the greatest number of missing values. In this table, the left columns show the names of the variables in the data set, while the columns 'n_miss' and 'pct_miss' represent the number of missing values and the percentage of missing values within each corresponding variable. It is evident that each of the top 10 variables has almost 100% missing values. A variable with a high percentage of missing values is not useful for analysis and should be ignored or removed from the data set. In fact, variables with more than 10% missing values should not be used in any analysis, especially in data sets with a large number of observations, as a high percentage of missing values can introduce bias into the analysis.

There are 91 variables in this data set with more than 10% missing values. These variables will be removed from the data set to ensure that the analysis in this project is based on variables with relatively completed data. This step will contribute to more accurate and less biased results in the survey method data quality analysis.

```{r}
# Summarise missings in variables
miss_var_list <- dat %>% naniar::miss_var_summary() 

miss_var_list <- miss_var_list[1:91,1] 

dat <- dat[, !(names(dat) %in% miss_var_list$variable)]

```

```{r}
dat1 <- dat %>% 
  mutate(prob_based = ifelse(DataSource <= 5, "Probablity based", "Non-probablity based")) %>% # Indicate probability based
  filter(DataSource %in% c(2, 5, 6, 7, 8, 9)) # Filter data source

```


```{r}
# Mutate new column with DataSource name
dat1 <- dat1 %>% 
  mutate(DataSource_name = case_when(
    DataSource == 2 ~ "Life in Australia",
    DataSource == 5 ~ "SMS push to web",
    DataSource == 6 ~ "Panel 1",
    DataSource == 7 ~ "Panel 2",
    DataSource == 8 ~ "Panel 3",
    DataSource == 9 ~ "Panel 4")) %>% 
  mutate(DataSource_name = factor(DataSource_name,
                       levels = c("Life in Australia",
                                  "SMS push to web",
                                  "Panel 1",
                                  "Panel 2",
                                  "Panel 3",
                                  "Panel 4"),
                       labels = c("Life in Australia",
                                  "SMS push to web",
                                  "Panel 1",
                                  "Panel 2",
                                  "Panel 3",
                                  "Panel 4"))) # Use str() to check levels
```

The variables with more than 10% missing values have been successfully removed from the dataset. As previously mentioned, the ACSSM dataset originally contained data from 8 surveys. However, I was instructed by the social research center not to use two of them. As a result, I am left with data from 6 surveys that can be used for comparing survey data quality. Out of these 6 surveys, two are probability-based, and four are non-probability-based. While the number of probability-based surveys is now less than the number of non-probability surveys, this discrepancy does not pose a problem because the goal is to compare the data quality between the surveys.

To facilitate the comparison and improve the clarity of the analysis, I have added two new columns to the dataset. The first column labels whether a respondent is from a probability-based survey or a non-probability-based survey. The second column contains the full names of the surveys, corresponding to the index numbers in the 'DataSource' variable. This is particularly useful because, in the ACSSM data, each survey is represented by digits in the 'DataSource' variable, which can be confusing at times. By adding this new column with corresponding survey names, it becomes easier to identify which survey a respondent belongs to. In this new column, "Life in Australia" and "SMS push to web" surveys are classified as probability-based, while "Panel 1", "Panel 2", "Panel 3", and "Panel 4" surveys are considered non-probability-based.

Now, after cleaning and organizing the data set, the new ACSSM data set contains 4,640 observations and 201 variables.

We have the cleaned ACSSM data set ready. Now, it's time to connect the metrics with the variables in the data set. To simplify the connection process, I have created an Excel spreadsheet. In this spreadsheet, I've listed all the questions from different sections of the survey, along with some potentially applicable metrics listed on the other side of the spreadsheet. Using Excel spreadsheet provides an overview of all survey questions, which will facilitate the connection between metrics and variables. You can find this Excel spreadsheet named "Survey_question.xlsx" in the "Project_related_material" folder.


# Chanllenges
- Understand the metrics
- matching the metrics to variables


# Analysis

### Response Time Analysis

```{r time1, fig.cap="Distributions of time spent answering the survey questions for panels."}
dat1 %>% 
  mutate("Section_SUM_main" = Section_Loop_Section4_SectionTimeMinutes + 
           Section_Loop_Section5_SectionTimeMinutes +
           Section_Loop_Section6_SectionTimeMinutes +
           Section_Loop_Section7_SectionTimeMinutes +
           Section_Loop_Section9_SectionTimeMinutes) %>% 
  ggplot(aes(x = DataSource_name,
                   y = Section_SUM_main,
                   color = prob_based)) +
  geom_point() + 
  geom_boxplot() + 
  # geom_hline(yintercept = 70 * 2 / 60, color = "purple", size = 1) +
  # geom_text(aes(2,70 * 2 / 60, label = "`Cutoff` Time: 2.33 mins", vjust = 2), show.legend = FALSE, color = "purple") +
  scale_color_brewer(palette = "Set1") +
  theme_classic() +
  theme(axis.text.x = element_text(angle=15, hjust=1)) +
  labs(x = "",
       y = "Total time in minutes",
       color = "")
```

The above Figure \@ref(fig:time1) illustrates the distributions of time consumed for answering survey questions for different panels. From Figure \@ref(fig:time1), the distributions are widely spread out; some respondents are using less time to complete the survey while some other respondent seems spend a long time to provide answers to the survey questions. 

```{r}
# distribution of page time for section 2, 4-9


dat1 %>% 
  mutate("Section_SUM_main" = Section_Loop_Section4_SectionTimeMinutes + 
           Section_Loop_Section5_SectionTimeMinutes +
           Section_Loop_Section6_SectionTimeMinutes +
           Section_Loop_Section7_SectionTimeMinutes +
           Section_Loop_Section9_SectionTimeMinutes) %>%
  filter(Section_SUM_main < 120) %>% 
  ggplot(aes(x = DataSource_name,
                   y = Section_SUM_main,
                   color = prob_based)) +
  geom_point() + 
  geom_boxplot() + 
  # geom_hline(yintercept = 70 * 2 / 60, color = "purple", size = 1) +
  # geom_text(aes(2,70 * 2 / 60, label = "`Cutoff` Time: 2.33 mins", vjust = 2), show.legend = FALSE, color = "purple") +
  scale_color_brewer(palette = "Set1") +
  theme_classic() +
  theme(axis.text.x = element_text(angle=15, hjust=1)) +
  labs(x = "",
       y = "Total time in minutes",
       color = "",
       title = "Distributions of time spent answering the survey questions for panels.")

dat1 %>% 
  mutate("Section_SUM_main" = Section_Loop_Section4_SectionTimeMinutes + 
           Section_Loop_Section5_SectionTimeMinutes +
           Section_Loop_Section6_SectionTimeMinutes +
           Section_Loop_Section7_SectionTimeMinutes +
           Section_Loop_Section9_SectionTimeMinutes) %>%
  filter(Section_SUM_main < 60) %>% 
  ggplot(aes(x = DataSource_name,
                   y = Section_SUM_main,
                   color = prob_based)) +
  geom_point() + 
  geom_boxplot() + 
  # geom_hline(yintercept = 70 * 2 / 60, color = "purple", size = 1) +
  # geom_text(aes(2,70 * 2 / 60, label = "`Cutoff` Time: 2.33 mins", vjust = 2), show.legend = FALSE, color = "purple") +
  scale_color_brewer(palette = "Set1") +
  theme_classic() +
  theme(axis.text.x = element_text(angle=15, hjust=1)) +
  labs(x = "",
       y = "Total time in minutes",
       color = "",
       title = "Distributions of time spent answering the survey questions for panels.")

dat1 %>% 
  mutate("Section_SUM_main" = Section_Loop_Section4_SectionTimeMinutes + 
           Section_Loop_Section5_SectionTimeMinutes +
           Section_Loop_Section6_SectionTimeMinutes +
           Section_Loop_Section7_SectionTimeMinutes +
           Section_Loop_Section9_SectionTimeMinutes) %>%
  filter(Section_SUM_main < 10) %>% 
  ggplot(aes(x = DataSource_name,
                   y = Section_SUM_main,
                   color = prob_based)) +
  geom_point() + 
  geom_boxplot() + 
  geom_hline(yintercept = 70 * 5 / 60, color = "purple", size = 1) +
  geom_text(aes(2,70 * 2 / 60, label = "Cutoff Time: 2.33 mins", vjust = -1), show.legend = FALSE, color = "purple") +
  scale_color_brewer(palette = "Set1") +
  theme_classic() +
  theme(axis.text.x = element_text(angle=15, hjust=1)) +
  labs(x = "",
       y = "Total time in minutes",
       color = "",
       title = "Distributions of time spent answering the survey questions for panels.")

```
# Limitation

# Summary

\newpage

# Reference

__Citation of R package__

Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL,
  Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019).
  “Welcome to the tidyverse.” _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686
  <https://doi.org/10.21105/joss.01686>.
  
Xie Y (2023). _bookdown: Authoring Books and Technical Documents with R Markdown_. R
  package version 0.35, <https://github.com/rstudio/bookdown>.
  
\newpage

# Appendix: All code for this report

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
